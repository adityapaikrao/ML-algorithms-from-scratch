{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63b3bac",
   "metadata": {},
   "source": [
    "## Why Learn Einstein Summation?\n",
    "Offers a concise, general way to specify nearly any product of tensors (vectors, matrices, higher-dimensional arrays).\n",
    "- Advantages over standard multiplications:\n",
    "  - Removes ambiguity over argument order, transpositions, and matching dimensions.\n",
    "  - User only needs to specify computation axes and desired output shape.\n",
    "\n",
    "Fun Facct: Despite its name, Einstein did not invent it. He popularized it by using it in his theory of General Relativity.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does `einsum()` Work?\n",
    "\n",
    "- **Interface:** `numpy.einsum` is called with a format string and any number of numpy tensors.\n",
    "  - Format: comma-separated specs for each argument, then an arrow (`->`), then the result spec.\n",
    "  - Each tensor’s spec uses a unique letter per axis (dim).\n",
    "- **Example:**  \n",
    "  - `\"ik,kj->ij\"` for matrix multiplication.\n",
    "- **Specs:**\n",
    "  - Each axis labeled with a single letter (ASCII).\n",
    "  - 0D tensors (scalars) use empty string `\"\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## Indices and Looping\n",
    "\n",
    "- **Free Indices:** Used in output spec—correspond to outer loops (determine output shape).\n",
    "- **Summation Indices:** Appear only in argument specs—correspond to inner loops (summed out in output).\n",
    "\n",
    "---\n",
    "\n",
    "## Rules and Pitfalls\n",
    "\n",
    "- \\# of specs = \\# of arguments.\n",
    "- Each spec length = tensor's dimension count.\n",
    "- Output indices must exist among input indices.\n",
    "- Using repeated indices in output only gives diagonal terms.\n",
    "- Having the same index for different axis lengths is not allowed.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Philosophy\n",
    "\n",
    "- Assign memorable labels to each axis.\n",
    "- Element-wise: same index for both tensors (`\"a,a->a\"`).\n",
    "- Summation on axis `'a'`: don’t put `'a'` in output spec (`\"a->\"`).\n",
    "- Inner product: element-wise, then sum (`\"a,a->\"`).\n",
    "- Input transpositions: handled automatically.\n",
    "- For output, just specify desired tensor form.\n",
    "\n",
    "---\n",
    "\n",
    "## Useful Format Strings\n",
    "\n",
    "- Vector inner product: `\"a,a->\"`\n",
    "- Vector element-wise: `\"a,a->a\"`\n",
    "- Vector outer: `\"a,b->ab\"`\n",
    "- Matrix transpose: `\"ab->ba\"`\n",
    "- Matrix diagonal: `\"ii->i\"`\n",
    "- Matrix trace: `\"ii->\"`\n",
    "- Matrix inner product: `\"ab,ab->\"`\n",
    "- Left-multiply matrix-vector: `\"ab,b->a\"`\n",
    "- Right-multiply vector-matrix: `\"a,ab->b\"`\n",
    "- Matrix multiplication: `\"ab,bc->ac\"`\n",
    "- Batch matrix multiply: `\"Yab,Ybc->Yac\"`\n",
    "- Quadratic form: `\"a,ab,b->\"`\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced & FAQ\n",
    "\n",
    "- If the output spec is omitted, numpy guesses free indices (those used only once in argument specs).\n",
    "- Advanced: `...` for broadcastable axes.\n",
    "- For complicated tensor contractions, contraction order matters for performance (see research on matrix chain ordering).\n",
    "- Sparse cases are much harder than dense; batching and BLAS can help only sometimes.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Numpy documentation: [einsum](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.einsum.html)\n",
    "- Example source code: [Pastebin link](http://pastebin.com/9GPvxwUn)\n",
    "- https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881b682",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
